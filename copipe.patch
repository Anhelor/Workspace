diff --git a/deepspeed/runtime/config.py b/deepspeed/runtime/config.py
index 975fb1f..4da5525 100755
--- a/deepspeed/runtime/config.py
+++ b/deepspeed/runtime/config.py
@@ -464,6 +464,7 @@ def get_pipeline_config(param_dict):
         "activation_checkpoint_interval": 0,
         "pipe_partitioned": True,
         "grad_partitioned": True,
+        "CoPipe_ratios": None,
     }
     config = default_pipeline
     for key, val in param_dict.get("pipeline", {}).items():
diff --git a/deepspeed/runtime/engine.py b/deepspeed/runtime/engine.py
index 5c1202b..7cdbb31 100644
--- a/deepspeed/runtime/engine.py
+++ b/deepspeed/runtime/engine.py
@@ -91,7 +91,7 @@ from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoin
 from .pipe.module import PipelineModule
 from .utils import get_ma_status
 from .compiler import CompiledModuleWrapper
-from ..ops.adam import FusedAdam
+from ..ops.adam import FusedAdam, DeepSpeedCPUAdam
 from ..moe.sharded_moe import TopKGate, MOELayer
 from ..moe.layer import MoE
 from ..moe.utils import is_moe_param
@@ -1414,9 +1414,9 @@ class DeepSpeedEngine(Module):
         dynamic_loss_args = self.dynamic_loss_scale_args()
         clip_grad = self.gradient_clipping()
         if APEX_INSTALLED:
-            fused_opts = (apex.optimizers.FusedAdam, FusedAdam)
+            fused_opts = (apex.optimizers.FusedAdam, FusedAdam, DeepSpeedCPUAdam)
         else:
-            fused_opts = FusedAdam
+            fused_opts = (FusedAdam, DeepSpeedCPUAdam)
         if isinstance(optimizer, fused_opts) \
                 or self.optimizer_name() in [ONEBIT_ADAM_OPTIMIZER, ZERO_ONE_ADAM_OPTIMIZER]:
             if self.dynamic_loss_scale():
diff --git a/deepspeed/runtime/fp16/fused_optimizer.py b/deepspeed/runtime/fp16/fused_optimizer.py
index 182f806..65066a4 100755
--- a/deepspeed/runtime/fp16/fused_optimizer.py
+++ b/deepspeed/runtime/fp16/fused_optimizer.py
@@ -11,12 +11,13 @@ import torch
 from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
 
 from deepspeed.runtime import DeepSpeedOptimizer
-from deepspeed.runtime.utils import get_global_norm, get_grad_norm, CheckOverflow, get_weight_norm, required_torch_version
+from deepspeed.runtime.utils import get_global_norm, see_memory_usage, get_local_grad_norm, CoPipeCheckOverflow, get_weight_norm, required_torch_version
 from deepspeed.runtime.fp16.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, MIN_LOSS_SCALE
 from deepspeed.utils import groups, logger, log_dist
 from deepspeed import comm as dist
 from deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT, CLIP_GRAD
 from deepspeed.accelerator import get_accelerator
+from deepspeed.ops.adam import FusedAdam, DeepSpeedCPUAdam
 
 OVERFLOW_CHECK_TIMER = 'overflow_check'
 COMPUTE_NORM_TIMER = 'compute_norm'
@@ -58,6 +59,18 @@ class FP16_Optimizer(DeepSpeedOptimizer):
             raise SystemError("Cannot use fp16 without accelerator.")
         self.optimizer = init_optimizer
 
+        # co-optimizer
+        self.accel = get_accelerator()
+        self.local_rank = dist.get_local_rank()
+        self.cpu_zero, self.cpu_one = -2, -1
+        self.cpu_ranks = [self.cpu_zero, self.cpu_one]
+        self.ratios = self.deepspeed._config.pipeline["CoPipe_ratios"][self.local_rank]
+        self.fp16_groups_by_rank = {rank: [] for rank in self.ratios}
+        self.fp16_groups_flat_by_rank = {rank: [] for rank in self.ratios}
+        self.fp32_groups_flat_by_rank = {rank: [] for rank in self.ratios}
+        self.opt_streams, self.recv_streams, self.opt_events, self.optimizers = {}, {}, {}, {}
+        assert abs(sum(self.ratios.values()) - 1) < 1e-9, "The sum of ratios should be 1"
+
         # param flattened by groups
         self.fp16_groups = []
         self.fp16_groups_flat = []
@@ -69,17 +82,43 @@ class FP16_Optimizer(DeepSpeedOptimizer):
         for i, param_group in enumerate(self.optimizer.param_groups):
             # push this group to list before modify
             self.fp16_groups.append(param_group['params'])
+            # partition fp16_groups
+            self._partition_params(i)
             # init fp16 weight buffer, flattened
-            self.fp16_groups_flat.append(_flatten_dense_tensors([p.clone().detach() for p in self.fp16_groups[i]]))
+            for rank, sg in sorted(self.fp16_groups_by_rank.items()):
+                self.fp16_groups_flat_by_rank[rank].append(_flatten_dense_tensors([p.clone().detach() for p in sg[i]]))
+            params_sg = [sg[i] for _, sg in sorted(self.fp16_groups_flat_by_rank.items())]
+            self.fp16_groups_flat.append(_flatten_dense_tensors(params_sg))
+            # set model fp16 subgroups weight to slices of flattened buffer
+            updated_params_sg = _unflatten_dense_tensors(self.fp16_groups_flat[i], params_sg)
+            for p, q in zip(params_sg, updated_params_sg):
+                p.data = q.data
             # set model fp16 weight to slices of flattened buffer
             updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])
             for p, q in zip(self.fp16_groups[i], updated_params):
                 p.data = q.data
             # init master weight, flattened
-            self.fp32_groups_flat.append(self.fp16_groups_flat[i].clone().float().detach())
+            for rank, sg in sorted(self.fp16_groups_flat_by_rank.items()):
+                if rank in self.cpu_ranks:
+                    self.fp32_groups_flat_by_rank[rank].append(sg[i].clone().to('cpu').float().detach().pin_memory())
+                else:
+                    self.fp32_groups_flat_by_rank[rank].append(sg[i].clone().to(rank).float().detach())
             # modify optimizer of have flat master weight
-            self.fp32_groups_flat[i].requires_grad = True  # keep this in case internal optimizer uses it
-            param_group['params'] = [self.fp32_groups_flat[i]]
+            for sg in self.fp32_groups_flat_by_rank.values():
+                sg[i].requires_grad = True  # keep this in case internal optimizer uses it
+            param_group['params'] = [torch.rand(1, requires_grad=True)]
+
+        # optimizer setup for each rank
+        for rank in sorted(self.ratios.keys()):
+            if rank in self.cpu_ranks:
+                self.optimizers[rank] = self._setup_optimizer(DeepSpeedCPUAdam, rank)
+            else:
+                self.opt_streams[rank] = self.accel.Stream(device=rank)
+                if rank != self.local_rank:
+                    self.recv_streams[rank] = self.accel.Stream(device=self.local_rank)
+                self.optimizers[rank] = self._setup_optimizer(FusedAdam, rank)
+            self.opt_events[rank] = self.accel.Event()
+        self.grad_stream = self.accel.Stream(device=self.local_rank)
 
         # we may have a way of fusing dynamic scale. Do not support for now
         if dynamic_loss_scale:
@@ -117,20 +156,98 @@ class FP16_Optimizer(DeepSpeedOptimizer):
         self.mpu = mpu
 
         self.overflow = False
-        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu, deepspeed=deepspeed)
+        self.overflow_checker = CoPipeCheckOverflow(self.fp16_groups, mpu=self.mpu, deepspeed=deepspeed)
+        see_memory_usage("Before initializing optimizer states", force=True)
         self.initialize_optimizer_states()
+        see_memory_usage("After initializing optimizer states", force=True)
+
+    def _partition_params(self, i):
+        total_numel = sum(p.numel() for p in self.fp16_groups[i])
+        curr_numel = 0
+        curr_ratio = 0
+        start_idx = 0
+        for rank, ratio in sorted(self.ratios.items()):
+            params_subgroups = []
+            curr_ratio += ratio
+            target_numel = round(total_numel * curr_ratio)
+            while curr_numel < target_numel and start_idx < len(self.fp16_groups[i]):
+                params_subgroups.append(self.fp16_groups[i][start_idx])
+                curr_numel += self.fp16_groups[i][start_idx].numel()
+                start_idx += 1
+            self.fp16_groups_by_rank[rank].append(params_subgroups)
+
+        last_rank1, last_rank0 = sorted(self.ratios.keys())[-2:]
+        if start_idx < len(self.fp16_groups[i]):
+            self.fp16_groups_by_rank[last_rank0][i] += self.fp16_groups[i][start_idx:]
+        if not self.fp16_groups_by_rank[last_rank0][i]:
+            self.fp16_groups_by_rank[last_rank0][i].append(self.fp16_groups_by_rank[last_rank1][i][-1])
+            self.fp16_groups_by_rank[last_rank1][i] = self.fp16_groups_by_rank[last_rank1][i][:-1]
+
+        partition_log = {-99: (total_numel, 1.00)}
+        for rank, sg in sorted(self.fp16_groups_by_rank.items()):
+            sg_numel = sum(p.numel() for p in sg[i])
+            partition_log[rank] = (len(sg[i]), sg_numel, round(sg_numel / float(total_numel), 3))
+        log_dist(f"parameters partitions: {partition_log}", ranks=list(range(dist.get_world_size())))
+
+    def _setup_optimizer(self, Optimizer, rank):
+        optimizer = Optimizer(
+            [self.fp32_groups_flat_by_rank[rank][0]], **{
+                k: self.optimizer.param_groups[0][k]
+                for k in ['lr', 'bias_correction', 'betas', 'eps', 'weight_decay', 'amsgrad']
+            })
+        if len(self.optimizer.param_groups) > 1:
+            for i in range(1, len(self.optimizer.param_groups)):
+                param_group = {
+                    k: [self.fp32_groups_flat_by_rank[rank][i]] if k == 'params' else self.optimizer.param_groups[i][k]
+                    for k in ['params', 'lr', 'bias_correction', 'betas', 'eps', 'weight_decay', 'amsgrad']
+                }
+                self.optimizers[rank].add_param_group(param_group)
+        return optimizer
+
+    def _backup_cpu_zero_optimizer_states(self):
+        self.os_backup = []
+        for g_id, g in enumerate(self.optimizers[self.cpu_zero].param_groups):
+            self.os_backup.append({'lr': g['lr'], 'params': [], 'step': [], 'exp_avg': [], 'exp_avg_sq': []})
+            for p_id, p in enumerate(g['params']):
+                self.os_backup[g_id]['params'].append(p.clone().detach())
+                state = self.optimizers[self.cpu_zero].state
+                self.os_backup[g_id]['step'].append(state[p]['step'])
+                self.os_backup[g_id]['exp_avg'].append(state[p]['exp_avg'].clone().detach())
+                self.os_backup[g_id]['exp_avg_sq'].append(state[p]['exp_avg_sq'].clone().detach())
+
+    def _rollback_cpu_zero_optimizer_states(self):
+        for g_id, g in enumerate(self.optimizers[self.cpu_zero].param_groups):
+            g['lr'] = self.os_backup[g_id]['lr']
+            for p_id, p in enumerate(g['params']):
+                p.data.copy_(self.os_backup[g_id]['params'][p_id].data)
+                state = self.optimizers[self.cpu_zero].state
+                state[p]['step'] = self.os_backup[g_id]['step'][p_id]
+                state[p]['exp_avg'].data.copy_(self.os_backup[g_id]['exp_avg'][p_id].data)
+                state[p]['exp_avg_sq'].data.copy_(self.os_backup[g_id]['exp_avg_sq'][p_id].data)
+
+    def _update_cpu_zero_optimizer_states_backup(self):
+        for g_id, g in enumerate(self.optimizers[self.cpu_zero].param_groups):
+            self.os_backup[g_id]['lr'] = g['lr']
+            for p_id, p in enumerate(g['params']):
+                self.os_backup[g_id]['params'][p_id].data.copy_(p.data)
+                state = self.optimizers[self.cpu_zero].state
+                self.os_backup[g_id]['step'][p_id] = state[p]['step']
+                self.os_backup[g_id]['exp_avg'][p_id].data.copy_(state[p]['exp_avg'].data)
+                self.os_backup[g_id]['exp_avg_sq'][p_id].data.copy_(state[p]['exp_avg_sq'].data)
 
     def initialize_optimizer_states(self):
         for i, group in enumerate(self.fp16_groups):
-            self.fp32_groups_flat[i].grad = torch.zeros(self.fp32_groups_flat[i].size(),
-                                                        device=self.fp32_groups_flat[i].device)
+            for rank, sg in sorted(self.fp32_groups_flat_by_rank.items()):
+                grad = torch.zeros(sg[i].size(), device=sg[i].device)
+                sg[i].grad = grad.pin_memory() if rank in self.cpu_ranks else grad
 
-        self.optimizer.step()
+        for rank in sorted(self.ratios.keys()):
+            self.optimizers[rank].step()
+            if rank not in self.cpu_ranks:
+                self._clean_fp32_grads(rank)
 
-        for i, group in enumerate(self.fp16_groups):
-            self.fp32_groups_flat[i].grad = None
-
-        return
+        if self.cpu_zero in self.ratios.keys():
+            self._backup_cpu_zero_optimizer_states()
 
     def zero_grad(self, set_to_none=True):
         """
@@ -205,6 +322,45 @@ class FP16_Optimizer(DeepSpeedOptimizer):
         self.custom_loss_scaler = True
         self.external_loss_scale = loss_scale
 
+    def _get_fp32_grads(self):
+        fp32_grads = []
+        grads_groups_by_rank = {rank: [] for rank in self.ratios}
+        for i, group in enumerate(self.fp16_groups):
+            for rank, sg in sorted(self.fp16_groups_by_rank.items()):
+                grads_subgroups = []
+                for p in sg[i]:
+                    grads_subgroups.append(
+                        torch.zeros(p.size(), dtype=torch.float, device=p.device) if p.grad is
+                        None else p.grad.float())
+                    p.grad = None
+                grads_groups_by_rank[rank].append(grads_subgroups)
+                fp32_grads.extend(grads_subgroups)
+        return fp32_grads, grads_groups_by_rank
+
+    def _set_fp32_grads(self, rank, src_ts):
+        for i in range(len(self.fp16_groups)):
+            sg = self.fp32_groups_flat_by_rank[rank]
+            if rank == self.local_rank:
+                sg[i].grad = src_ts[rank][i]
+            else:
+                if rank not in self.cpu_ranks:
+                    sg[i].grad = torch.empty(sg[i].size(), device=sg[i].device)
+                sg[i].grad.copy_(src_ts[rank][i], non_blocking=True)
+
+    def _clean_fp32_grads(self, rank):
+        for i in range(len(self.fp16_groups)):
+            self.fp32_groups_flat_by_rank[rank][i].grad = None
+
+    def _update_fp16_params(self, rank, src_ts=None):
+        for i in range(len(self.fp16_groups)):
+            sg = self.fp32_groups_flat_by_rank[rank]
+            if rank == self.local_rank:
+                self.fp16_groups_flat_by_rank[rank][i].data.copy_(sg[i].data, non_blocking=True)
+            else:
+                non_blocking = rank not in self.cpu_ranks
+                src_ts[rank][i].copy_(sg[i], non_blocking=non_blocking)
+                self.fp16_groups_flat_by_rank[rank][i].data.copy_(src_ts[rank][i].data, non_blocking=non_blocking)
+
     def step(self, closure=None):
         """
         Not supporting closure.
@@ -213,13 +369,43 @@ class FP16_Optimizer(DeepSpeedOptimizer):
         if self.fused_adam_legacy:
             return self.step_fused_adam()
 
-        # First determine if there is overflow.
-        self.timers(OVERFLOW_CHECK_TIMER).start()
-        fp16_params = []
-        for i, group in enumerate(self.fp16_groups):
-            fp16_params.extend([p for p in group if p.grad is not None])
-        self.overflow = self.overflow_checker.has_overflow(fp16_params)
-        self.timers(OVERFLOW_CHECK_TIMER).stop()
+        if not self.overflow and self.cpu_zero in self.ratios.keys():
+            self._update_cpu_zero_optimizer_states_backup()
+        self.accel.current_stream().synchronize()
+
+        fp32_grads, grads_groups_by_rank = self._get_fp32_grads()
+        local_overflow, overflow_gpu, overflow_work = self.overflow_checker.has_overflow(fp32_grads)
+        del fp32_grads
+
+        if not local_overflow:
+            grads_groups_flat_by_rank = {rank: [] for rank in self.ratios}
+            for i, group in enumerate(self.fp16_groups):
+                for rank, sg in sorted(grads_groups_by_rank.items()):
+                    grads_groups_flat_by_rank[rank].append(_flatten_dense_tensors(sg[i]))
+                    sg[i] = None
+            del grads_groups_by_rank
+
+            all_groups_norm = get_local_grad_norm(self.fp32_groups_flat_by_rank,
+                                                  grads_groups_flat_by_rank,
+                                                  mpu=self.mpu)
+            if self.has_moe_layers:
+                all_groups_norm = self._get_norm_with_moe_layers(all_groups_norm)
+            scaled_global_grad_norm = get_global_norm(norm_list=[all_groups_norm])
+            self._global_grad_norm = scaled_global_grad_norm / self.cur_scale
+            self.unscale_and_clip_grads(grads_groups_flat_by_rank, scaled_global_grad_norm)
+            self.accel.current_stream().synchronize()
+
+            for rank in sorted(self.ratios.keys()):
+                with self.accel.stream(self.grad_stream):
+                    self._set_fp32_grads(rank, grads_groups_flat_by_rank)
+                    if rank != self.local_rank:
+                        self.opt_events[rank].record()
+            if (rank := self.cpu_zero) in self.ratios.keys():
+                self.opt_events[rank].synchronize()
+                self.optimizers[rank].step()
+
+        overflow_work.wait()
+        self.overflow = bool(overflow_gpu[0].item())
         prev_scale = self.cur_scale
         self._update_scale(self.overflow)
         if self.overflow:
@@ -228,65 +414,33 @@ class FP16_Optimizer(DeepSpeedOptimizer):
                     "Overflow detected. Skipping step. Attempted loss "
                     f"scale: {prev_scale}, reducing to {self.cur_scale}",
                     ranks=[0])
-            # Clear gradients
-            for i, group in enumerate(self.fp16_groups):
-                for p in group:
-                    p.grad = None
 
-            self.timers.log(OVERFLOW_TIMERS)
+            if not local_overflow and self.cpu_zero in self.ratios.keys():
+                self._rollback_cpu_zero_optimizer_states()
             return self.overflow
 
-        grads_groups_flat = []
-        for i, group in enumerate(self.fp16_groups):
-            data_type = self.fp32_groups_flat[i].dtype
-
-            grads_groups_flat.append(
-                _flatten_dense_tensors([
-                    torch.zeros(p.size(), dtype=data_type, device=p.device) if p.grad is None else p.grad.to(data_type)
-                    for p in group
-                ]))
-
-            for p in group:
-                p.grad = None
-
-            self.fp32_groups_flat[i].grad = grads_groups_flat[i]
-
-        self.timers(COMPUTE_NORM_TIMER).start()
-
-        all_groups_norm = get_grad_norm(self.fp32_groups_flat, mpu=self.mpu)
-
-        self.timers(COMPUTE_NORM_TIMER).stop()
-
-        if self.has_moe_layers:
-            all_groups_norm = self._get_norm_with_moe_layers(all_groups_norm)
-
-        scaled_global_grad_norm = get_global_norm(norm_list=[all_groups_norm])
-
-        # Stash unscaled gradient norm
-        self._global_grad_norm = scaled_global_grad_norm / self.cur_scale
-
-        self.timers(UNSCALE_AND_CLIP_TIMER).start()
-        self.unscale_and_clip_grads(grads_groups_flat, scaled_global_grad_norm)
-        self.timers(UNSCALE_AND_CLIP_TIMER).stop()
-
-        self.timers(BASIC_STEP_TIMER).start()
-        self.optimizer.step()
-        self.timers(BASIC_STEP_TIMER).stop()
-
-        #get rid of the fp32 gradients. Not needed anymore
-        for group in self.fp32_groups_flat:
-            group.grad = None
-
-        self.timers(UPDATE_FP16_TIMER).start()
-
-        for i in range(len(self.fp16_groups)):
-            updated_params = _unflatten_dense_tensors(self.fp32_groups_flat[i], self.fp16_groups[i])
-            for p, q in zip(self.fp16_groups[i], updated_params):
-                p.data.copy_(q.data)
-
-        self.timers(UPDATE_FP16_TIMER).stop()
-
-        self.timers.log(STEP_TIMERS)
+        for rank in sorted(self.ratios.keys(), reverse=True):
+            if rank in self.cpu_ranks:
+                if rank == self.cpu_one:
+                    self.opt_events[rank].synchronize()
+                    self.optimizers[rank].step()
+                self._update_fp16_params(rank, grads_groups_flat_by_rank)
+            else:
+                with self.accel.stream(self.opt_streams[rank]) as opt_stream:
+                    if rank == self.local_rank:
+                        self.optimizers[rank].step()
+                        self._clean_fp32_grads(rank)
+                        self._update_fp16_params(rank)
+                        self.opt_events[rank].record()
+                    else:
+                        self.opt_events[rank].wait(opt_stream)
+                        self.optimizers[rank].step()
+                        self._clean_fp32_grads(rank)
+                        with self.accel.stream(self.recv_streams[rank]) as recv_stream:
+                            self._update_fp16_params(rank, grads_groups_flat_by_rank)
+                            self.opt_events[rank].record(recv_stream)
+        for event in self.opt_events.values():
+            event.synchronize()
 
         return self.overflow
 
@@ -314,8 +468,9 @@ class FP16_Optimizer(DeepSpeedOptimizer):
                 combined_scale = clip * self.cur_scale
 
         if apply_scale:
-            for grad in grad_groups_flat:
-                grad.data.mul_(1. / combined_scale)
+            for sg in grad_groups_flat.values():
+                for g in sg:
+                    g.data.mul_(1. / combined_scale)
 
         return combined_scale
 
diff --git a/deepspeed/runtime/pipe/engine.py b/deepspeed/runtime/pipe/engine.py
index ef1c98a..e584dd6 100644
--- a/deepspeed/runtime/pipe/engine.py
+++ b/deepspeed/runtime/pipe/engine.py
@@ -371,7 +371,7 @@ class PipelineEngine(DeepSpeedEngine):
                                        stages=self.num_stages,
                                        stage_id=self.stage_id)
         self._exec_schedule(sched)
-        self.agg_train_loss = self._aggregate_total_loss()
+        self.agg_train_loss = self._aggregate_total_loss_copipe()
 
         self.timers(TRAIN_BATCH_TIMER).stop()
 
@@ -381,17 +381,19 @@ class PipelineEngine(DeepSpeedEngine):
                 iter_time = elapsed / self.steps_per_print()
                 tput = self.train_batch_size() / iter_time
                 print(f'steps: {self.global_steps} '
-                      f'loss: {self.agg_train_loss:0.4f} '
                       f'iter time (s): {iter_time:0.3f} '
                       f'samples/sec: {tput:0.3f}')
             else:
                 self.timers(TRAIN_BATCH_TIMER).elapsed(reset=True)
+                if self.is_last_stage():
+                    print(f'steps: {self.global_steps} '
+                          f'loss: {self.agg_train_loss:0.4f} ')
 
         # Monitoring
-        if self.global_rank == 0 and self.monitor.enabled:
-            self.summary_events = [(f'Train/Samples/train_loss', self.agg_train_loss.mean().item(),
-                                    self.global_samples)]
-            self.monitor.write_events(self.summary_events)
+        # if self.global_rank == 0 and self.monitor.enabled:
+        #     self.summary_events = [(f'Train/Samples/train_loss', self.agg_train_loss.mean().item(),
+        #                             self.global_samples)]
+        #     self.monitor.write_events(self.summary_events)
 
         if self.wall_clock_breakdown() and self.global_steps % self.steps_per_print() == 0:
             self.timers.log([
@@ -590,6 +592,24 @@ class PipelineEngine(DeepSpeedEngine):
 
         return agg_loss
 
+    def _aggregate_total_loss_copipe(self):
+        if self.is_last_stage():
+            loss = self._scale_loss_by_gas(self.total_loss)
+            self.dp_group_loss = loss.clone().detach()
+
+            agg_loss = self.dp_group_loss.clone().detach()
+            if self.is_data_parallel:
+                dist.all_reduce(agg_loss, group=self.mpu.get_data_parallel_group())
+                agg_loss /= self.dp_world_size
+
+            assert self.global_rank in self.grid.pp_group
+            losses = torch.stack([self.dp_group_loss, agg_loss]).float()
+            self.dp_group_loss = losses[0].clone().detach()
+            agg_loss = losses[1].clone().detach()
+            return agg_loss
+        else:
+            return None
+
     def set_dataloader(self, loader):
         """"""
         if self.is_first_stage() or self.is_last_stage():
@@ -710,6 +730,7 @@ class PipelineEngine(DeepSpeedEngine):
             if self._compute_loss and self.module.loss_fn is not None:
                 labels = self.pipe_buffers['labels'][buffer_id]
                 self.loss = self.module.loss_fn(outputs, labels)
+                self.pipe_buffers['outputs'][buffer_id] = None
             else:
                 # Some models just return loss from forward()
                 self.loss = outputs
diff --git a/deepspeed/runtime/pipe/schedule.py b/deepspeed/runtime/pipe/schedule.py
index 21bf24d..f51eb23 100644
--- a/deepspeed/runtime/pipe/schedule.py
+++ b/deepspeed/runtime/pipe/schedule.py
@@ -236,8 +236,6 @@ class TrainSchedule(PipeSchedule):
 
             # Model step at the end of the batch
             if step_id == total_steps - 1:
-                cmds.append(ReduceTiedGrads())
-                cmds.append(ReduceGrads())
                 cmds.append(OptimizerStep())
 
             # Prepare state for next time
diff --git a/deepspeed/runtime/utils.py b/deepspeed/runtime/utils.py
index d1ebe4b..41842f4 100755
--- a/deepspeed/runtime/utils.py
+++ b/deepspeed/runtime/utils.py
@@ -317,6 +317,43 @@ class CheckOverflow(object):
             return False
 
 
+class CoPipeCheckOverflow(CheckOverflow):
+
+    def has_overflow_serial(self, grads):
+        all_sums = [g.data.sum() for g in grads]
+        total_sum = torch.stack(all_sums)
+
+        inf = total_sum.isinf()
+        nan = total_sum.isnan()
+        inf_or_nan = inf.logical_or(nan)
+
+        return bool(inf_or_nan.max())
+
+    def has_overflow(self, grads, has_moe_params=None):
+        if has_moe_params is None:
+            has_moe_params = self.has_moe_params
+        overflow = self.has_overflow_serial(grads)
+        overflow_gpu = get_accelerator().ByteTensor([overflow])
+        if has_moe_params:
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=groups._get_max_expert_parallel_group())
+        if self.zero_reduce_scatter:
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=dist.get_world_group())
+        elif self.mpu is not None:
+            if self.deepspeed is not None:
+                using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')
+                if (using_pipeline and self.deepspeed.pipeline_enable_backward_allreduce
+                        is False) or (not using_pipeline and self.deepspeed.enable_backward_allreduce is False):
+                    dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_data_parallel_group())
+            work = dist.all_reduce(overflow_gpu,
+                                   op=dist.ReduceOp.MAX,
+                                   group=self.mpu.get_model_parallel_group(),
+                                   async_op=True)
+        elif self.deepspeed is not None and self.deepspeed.enable_backward_allreduce is False:
+            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=dist.get_world_group())
+
+        return overflow, overflow_gpu, work
+
+
 def _handle_overflow(cpu_sum, x, i):
     import math
     rank = dist.get_rank()
@@ -469,6 +506,35 @@ def get_grad_norm(parameters, norm_type=2, mpu=None):
     return total_norm
 
 
+def get_local_grad_norm(params, grads, norm_type=2, mpu=None):
+    """Get grad norm of an iterable of parameters.
+    Returns:
+        Total norm of the gradients
+    """
+    assert norm_type == 2, "only L2 norm supported"
+    norm_type = float(norm_type)
+    all_norms = []
+    tensor_mp_rank = bwc_tensor_model_parallel_rank(mpu=mpu)
+    for (_, params_sg), (_, grads_sg) in zip(sorted(params.items()), sorted(grads.items())):
+        for p, g in zip(params_sg, grads_sg):
+            if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:
+                continue
+            if (tensor_mp_rank > 0) and not is_model_parallel_parameter(p):
+                continue
+            all_norms.append(torch.norm(g.data.detach(), norm_type).to(get_accelerator().current_device_name()))
+    if len(all_norms) > 0:
+        total_norm = torch.stack(all_norms).square().sum()
+    else:
+        total_norm = torch.tensor(0.0, dtype=torch.float32).to(get_accelerator().current_device_name())
+    total_norm = total_norm.pow(1. / norm_type)
+    total_norm = total_norm.item()
+
+    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
+        total_norm = -1
+
+    return total_norm
+
+
 def get_grad_zeros(parameters, mpu=None):
     """Compute the number of grads with zero values.
 
@@ -808,6 +874,7 @@ def see_memory_usage(message, force=False):
     logger.info(f'CPU Virtual Memory:  used = {used_GB} GB, percent = {vm_stats.percent}%')
 
     # get the peak memory to report correct data, so reset the counter for the next call
+    get_accelerator().empty_cache()
     get_accelerator().reset_peak_memory_stats()
 
 
